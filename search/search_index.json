{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Networks","text":""},{"location":"#what-is-this","title":"What is this?","text":"<p>This is a personal project to learn about neural networks and how they work. I will be using this repository to implement various neural networks from scratch (using Numpy and Pytorch) and to document my learning process.</p>"},{"location":"tests/","title":"tests","text":""},{"location":"concepts/ff/","title":"Neural Network Implementation","text":"<p>This code defines a simple feedforward neural network with several layers and activation functions. Below is a brief explanation of the code and a check for any mistakes.</p>"},{"location":"concepts/ff/#code-overview","title":"Code Overview","text":"<ol> <li> <p>The code imports the <code>torch</code> library for deep learning operations and defines some necessary types and functions.</p> </li> <li> <p>It defines four classes:</p> </li> <li> <p><code>Linear</code>: Represents a linear layer in the neural network with weights and biases. It initializes the weights using Glorot initialization and can perform a forward pass.</p> </li> <li><code>Sigmoid</code>: Represents the sigmoid activation function. It has a forward pass and a method to compute its gradient.</li> <li><code>TanH</code>: Represents the hyperbolic tangent (tanh) activation function. It also has a forward pass and a method to compute its gradient.</li> <li> <p><code>MSELoss</code>: Represents the mean squared error loss function, commonly used for regression tasks. It has a forward pass to compute the loss and a method to compute its gradient.</p> </li> <li> <p>The <code>NeuralLayer</code> class defines a layer in the neural network, which includes a linear transformation and an activation function (either sigmoid or tanh). It also tracks the input, intermediate values, and output values for later use in the backward pass.</p> </li> <li> <p>The <code>NeuralNetwork</code> class defines the neural network architecture. It consists of multiple <code>NeuralLayer</code> instances, and the architecture is specified by the number of input and output units, as well as a list of hidden layer sizes. The network uses tanh activation for hidden layers and sigmoid activation for the output layer. It includes methods for forward pass, loss computation, backward pass, gradient computation, weight updates, training, and prediction.</p> </li> </ol>"},{"location":"concepts/ff/#usage","title":"Usage","text":"<p>The code appears to be free of syntax errors. However, it's important to note that the network architecture is fixed with tanh for hidden layers and sigmoid for the output layer, and the Glorot initialization method is used for weight initialization. You may need to customize it for your specific use case if you require different activations or weight initialization techniques.</p>"},{"location":"doc/linear/","title":"Feed Forward","text":""},{"location":"doc/linear/#networks.Linear.feed_forward.Linear","title":"<code>Linear</code>","text":"<p>A linear layer in a neural network.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class Linear:\n    \"\"\"\n    A linear layer in a neural network.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int):\n        self.weight = self._init_glorot(in_features, out_features)\n        self.bias = torch.zeros(out_features)\n\n        self.weight_grad: Optional[Tensor] = None\n        self.bias_grad: Optional[Tensor] = None\n\n    @staticmethod\n    def _init_glorot(in_features: int, out_features: int) -&gt; Tensor:\n        \"\"\"\n        Initialize a weight matrix with glorot initialization.\n\n        Args:\n            in_features (int): Number of input features.\n            out_features (int): Number of output features.\n\n        Returns:\n            weight_matrix (Tensor): A weight matrix with glorot initialization.\n        \"\"\"\n        b = torch.sqrt(torch.tensor([6. / (in_features + out_features)]))\n        return (2 * b) * torch.rand(in_features, out_features) - b\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        return x @ self.weight + self.bias\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.Linear.forward","title":"<code>forward(x)</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    return x @ self.weight + self.bias\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.MSELoss","title":"<code>MSELoss</code>","text":"<p>A mean squared error loss function.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class MSELoss:\n    \"\"\"\n    A mean squared error loss function.\n    \"\"\"\n    @staticmethod\n    def forward(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            y_true (Tensor): True labels.\n            y_pred (Tensor): Predicted labels.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        return torch.mean((y_true - y_pred)**2)\n\n    @staticmethod\n    def get_gradient(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the gradient of the loss function.\n\n        Args:\n            y_true (Tensor): True labels.\n            y_pred (Tensor): Predicted labels.\n\n        Returns:\n            gradient (Tensor): Gradient tensor.\n        \"\"\"\n        return 2 * (y_pred - y_true) / len(y_true)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.MSELoss.forward","title":"<code>forward(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>True labels.</p> required <code>y_pred</code> <code>Tensor</code> <p>Predicted labels.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>@staticmethod\ndef forward(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        y_true (Tensor): True labels.\n        y_pred (Tensor): Predicted labels.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    return torch.mean((y_true - y_pred)**2)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.MSELoss.get_gradient","title":"<code>get_gradient(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Compute the gradient of the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>True labels.</p> required <code>y_pred</code> <code>Tensor</code> <p>Predicted labels.</p> required <p>Returns:</p> Name Type Description <code>gradient</code> <code>Tensor</code> <p>Gradient tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>@staticmethod\ndef get_gradient(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the gradient of the loss function.\n\n    Args:\n        y_true (Tensor): True labels.\n        y_pred (Tensor): Predicted labels.\n\n    Returns:\n        gradient (Tensor): Gradient tensor.\n    \"\"\"\n    return 2 * (y_pred - y_true) / len(y_true)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer","title":"<code>NeuralLayer</code>","text":"<p>A neural layer in a neural network.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class NeuralLayer:\n    \"\"\"\n    A neural layer in a neural network.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, activation: str):\n        \"\"\"\n        Initialize the linear layer and activation function.\n\n        Args:\n            in_features (int): Number of input features.\n            out_features (int): Number of output features.\n            activation (str): Activation function.\n        \"\"\"\n        self.linear = Linear(in_features, out_features)\n\n        if activation == 'sigmoid':\n            self.act = Sigmoid()\n        elif activation == 'tanh':\n            self.act = TanH()\n        else:\n            raise ValueError('{} activation is unknown'.format(activation))\n\n        # We save the last computation as we'll need it for the backward pass.\n        self.last_input: Optional[None] = None\n        self.last_zin: Optional[None] = None\n        self.last_zout: Optional[None] = None\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        self.last_input = x\n        self.last_zin = self.linear.forward(x)\n        self.last_zout = self.act.forward(self.last_zin)\n        return self.last_zout\n\n    def get_weight(self) -&gt; Tensor:\n        \"\"\"\n        Get the weight matrix in the linear layer.\n\n        Returns:\n            weight_matrix (Tensor): Weight matrix.\n        \"\"\"\n        return self.linear.weight\n\n    def get_bias(self) -&gt; Tensor:\n        \"\"\"\n        Get the bias in the linear layer.\n\n        Returns:\n            bias (Tensor): Bias.\n        \"\"\"\n        return self.linear.bias\n\n    def set_weight_gradient(self, grad: Tensor) -&gt; None:\n        \"\"\"\n        Set a tensor as gradient for the weight in the linear layer.\n\n        Args:\n            grad (Tensor): Gradient tensor.\n        \"\"\"\n        self.linear.weight_grad = grad\n\n    def set_bias_gradient(self, grad: Tensor) -&gt; None:\n        \"\"\"\n        Set a tensor as gradient for the bias in the linear layer.\n\n        Args:\n            grad (Tensor): Gradient tensor.\n        \"\"\"\n        self.linear.bias_grad = grad\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.__init__","title":"<code>__init__(in_features, out_features, activation)</code>","text":"<p>Initialize the linear layer and activation function.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features.</p> required <code>out_features</code> <code>int</code> <p>Number of output features.</p> required <code>activation</code> <code>str</code> <p>Activation function.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def __init__(self, in_features: int, out_features: int, activation: str):\n    \"\"\"\n    Initialize the linear layer and activation function.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        activation (str): Activation function.\n    \"\"\"\n    self.linear = Linear(in_features, out_features)\n\n    if activation == 'sigmoid':\n        self.act = Sigmoid()\n    elif activation == 'tanh':\n        self.act = TanH()\n    else:\n        raise ValueError('{} activation is unknown'.format(activation))\n\n    # We save the last computation as we'll need it for the backward pass.\n    self.last_input: Optional[None] = None\n    self.last_zin: Optional[None] = None\n    self.last_zout: Optional[None] = None\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.forward","title":"<code>forward(x)</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    self.last_input = x\n    self.last_zin = self.linear.forward(x)\n    self.last_zout = self.act.forward(self.last_zin)\n    return self.last_zout\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.get_bias","title":"<code>get_bias()</code>","text":"<p>Get the bias in the linear layer.</p> <p>Returns:</p> Name Type Description <code>bias</code> <code>Tensor</code> <p>Bias.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def get_bias(self) -&gt; Tensor:\n    \"\"\"\n    Get the bias in the linear layer.\n\n    Returns:\n        bias (Tensor): Bias.\n    \"\"\"\n    return self.linear.bias\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.get_weight","title":"<code>get_weight()</code>","text":"<p>Get the weight matrix in the linear layer.</p> <p>Returns:</p> Name Type Description <code>weight_matrix</code> <code>Tensor</code> <p>Weight matrix.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def get_weight(self) -&gt; Tensor:\n    \"\"\"\n    Get the weight matrix in the linear layer.\n\n    Returns:\n        weight_matrix (Tensor): Weight matrix.\n    \"\"\"\n    return self.linear.weight\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.set_bias_gradient","title":"<code>set_bias_gradient(grad)</code>","text":"<p>Set a tensor as gradient for the bias in the linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>grad</code> <code>Tensor</code> <p>Gradient tensor.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def set_bias_gradient(self, grad: Tensor) -&gt; None:\n    \"\"\"\n    Set a tensor as gradient for the bias in the linear layer.\n\n    Args:\n        grad (Tensor): Gradient tensor.\n    \"\"\"\n    self.linear.bias_grad = grad\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralLayer.set_weight_gradient","title":"<code>set_weight_gradient(grad)</code>","text":"<p>Set a tensor as gradient for the weight in the linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>grad</code> <code>Tensor</code> <p>Gradient tensor.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def set_weight_gradient(self, grad: Tensor) -&gt; None:\n    \"\"\"\n    Set a tensor as gradient for the weight in the linear layer.\n\n    Args:\n        grad (Tensor): Gradient tensor.\n    \"\"\"\n    self.linear.weight_grad = grad\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork","title":"<code>NeuralNetwork</code>","text":"<p>A linear feedforward neural network.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class NeuralNetwork:\n    \"\"\"\n    A linear feedforward neural network.\n    \"\"\"\n    def __init__(self, input_size, output_size, hidden_sizes: List[int]):\n        \"\"\"\n        Initialize the neural network.\n\n        Args:\n            input_size (int): Number of input features.\n            output_size (int): Number of output features.\n            hidden_sizes (List[int]): List of hidden layer sizes.\n        \"\"\"\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_sizes = hidden_sizes\n\n        self.layers: List[NeuralLayer] = []\n        layer_sizes = [self.input_size] + self.hidden_sizes\n        for i in range(1, len(layer_sizes)):\n            self.layers.append(NeuralLayer(layer_sizes[i-1], layer_sizes[i], 'tanh'))\n        self.layers.append(NeuralLayer(hidden_sizes[-1], self.output_size, 'sigmoid'))\n\n        self.loss = MSELoss()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n\n    def get_loss(self, x: Tensor, y: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the loss for a dataset and given labels.\n\n        Args:\n            x (Tensor): Input tensor.\n            y (Tensor): Labels.\n\n        Returns:\n            loss (Tensor): Loss tensor.\n        \"\"\"\n        return self.loss.forward(self.forward(x), y)\n\n    def backward(self, x: Tensor, y: Tensor) -&gt; None:\n        \"\"\"\n        Compute the backward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n            y (Tensor): Labels.\n        \"\"\"\n        y_pred = self.forward(x)\n        grad = self.loss.get_gradient(y, y_pred)\n        for layer in reversed(self.layers):\n            grad = grad * layer.act.get_gradient(layer.last_zin)\n            layer.set_weight_gradient(layer.last_input.T @ grad)\n            layer.set_bias_gradient(torch.sum(grad, dim=0))\n            grad = grad @ layer.get_weight().T\n\n        # Check if gradients have the right size.\n        for i, layer in enumerate(self.layers):\n            if layer.linear.weight_grad.shape != layer.linear.weight.shape \\\n                or layer.linear.bias_grad.shape != layer.linear.bias.shape:\n                raise ValueError('Gradients in layer with index {} have a wrong shape.'\n                                 .format(i))\n\n\n    def apply_gradients(self, learning_rate: float) -&gt; None:\n        \"\"\"\n        Update weights with the computed gradients.\n\n        Args:\n            learning_rate (float): Learning rate.\n        \"\"\"\n        for layer in self.layers:\n            layer.linear.weight -= learning_rate * layer.linear.weight_grad\n            layer.linear.bias -= learning_rate * layer.linear.bias_grad\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork.__init__","title":"<code>__init__(input_size, output_size, hidden_sizes)</code>","text":"<p>Initialize the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Number of input features.</p> required <code>output_size</code> <code>int</code> <p>Number of output features.</p> required <code>hidden_sizes</code> <code>List[int]</code> <p>List of hidden layer sizes.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def __init__(self, input_size, output_size, hidden_sizes: List[int]):\n    \"\"\"\n    Initialize the neural network.\n\n    Args:\n        input_size (int): Number of input features.\n        output_size (int): Number of output features.\n        hidden_sizes (List[int]): List of hidden layer sizes.\n    \"\"\"\n    self.input_size = input_size\n    self.output_size = output_size\n    self.hidden_sizes = hidden_sizes\n\n    self.layers: List[NeuralLayer] = []\n    layer_sizes = [self.input_size] + self.hidden_sizes\n    for i in range(1, len(layer_sizes)):\n        self.layers.append(NeuralLayer(layer_sizes[i-1], layer_sizes[i], 'tanh'))\n    self.layers.append(NeuralLayer(hidden_sizes[-1], self.output_size, 'sigmoid'))\n\n    self.loss = MSELoss()\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork.apply_gradients","title":"<code>apply_gradients(learning_rate)</code>","text":"<p>Update weights with the computed gradients.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def apply_gradients(self, learning_rate: float) -&gt; None:\n    \"\"\"\n    Update weights with the computed gradients.\n\n    Args:\n        learning_rate (float): Learning rate.\n    \"\"\"\n    for layer in self.layers:\n        layer.linear.weight -= learning_rate * layer.linear.weight_grad\n        layer.linear.bias -= learning_rate * layer.linear.bias_grad\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork.backward","title":"<code>backward(x, y)</code>","text":"<p>Compute the backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor</code> <p>Labels.</p> required Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def backward(self, x: Tensor, y: Tensor) -&gt; None:\n    \"\"\"\n    Compute the backward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n        y (Tensor): Labels.\n    \"\"\"\n    y_pred = self.forward(x)\n    grad = self.loss.get_gradient(y, y_pred)\n    for layer in reversed(self.layers):\n        grad = grad * layer.act.get_gradient(layer.last_zin)\n        layer.set_weight_gradient(layer.last_input.T @ grad)\n        layer.set_bias_gradient(torch.sum(grad, dim=0))\n        grad = grad @ layer.get_weight().T\n\n    # Check if gradients have the right size.\n    for i, layer in enumerate(self.layers):\n        if layer.linear.weight_grad.shape != layer.linear.weight.shape \\\n            or layer.linear.bias_grad.shape != layer.linear.bias.shape:\n            raise ValueError('Gradients in layer with index {} have a wrong shape.'\n                             .format(i))\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork.forward","title":"<code>forward(x)</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    for layer in self.layers:\n        x = layer.forward(x)\n    return x\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.NeuralNetwork.get_loss","title":"<code>get_loss(x, y)</code>","text":"<p>Compute the loss for a dataset and given labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor</code> <p>Labels.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>Tensor</code> <p>Loss tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def get_loss(self, x: Tensor, y: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the loss for a dataset and given labels.\n\n    Args:\n        x (Tensor): Input tensor.\n        y (Tensor): Labels.\n\n    Returns:\n        loss (Tensor): Loss tensor.\n    \"\"\"\n    return self.loss.forward(self.forward(x), y)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.Sigmoid","title":"<code>Sigmoid</code>","text":"<p>A sigmoid activation function.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class Sigmoid:\n    \"\"\"\n    A sigmoid activation function.  \n    \"\"\"\n    def __init__(self):\n        self.func = lambda x: 1 / (1 + torch.exp(-x))\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        return self.func(x)\n\n    def get_gradient(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the gradient of the sigmoid function.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            gradient (Tensor): Gradient tensor.\n        \"\"\"\n        return self.func(x) * (1 - self.func(x))\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.Sigmoid.forward","title":"<code>forward(x)</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    return self.func(x)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.Sigmoid.get_gradient","title":"<code>get_gradient(x)</code>","text":"<p>Compute the gradient of the sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>gradient</code> <code>Tensor</code> <p>Gradient tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>def get_gradient(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the gradient of the sigmoid function.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        gradient (Tensor): Gradient tensor.\n    \"\"\"\n    return self.func(x) * (1 - self.func(x))\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.TanH","title":"<code>TanH</code>","text":"<p>A tanh activation function.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>class TanH:\n    \"\"\"\n    A tanh activation function.\n    \"\"\"\n    @staticmethod\n    def forward(x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the forward pass.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            output (Tensor): Output tensor.\n        \"\"\"\n        return torch.tanh(x)\n\n    @staticmethod\n    def get_gradient(x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the gradient of the tanh function.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            gradient (Tensor): Gradient tensor.\n        \"\"\"\n        return  1 - torch.tanh(x)**2\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.TanH.forward","title":"<code>forward(x)</code>  <code>staticmethod</code>","text":"<p>Compute the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>@staticmethod\ndef forward(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the forward pass.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        output (Tensor): Output tensor.\n    \"\"\"\n    return torch.tanh(x)\n</code></pre>"},{"location":"doc/linear/#networks.Linear.feed_forward.TanH.get_gradient","title":"<code>get_gradient(x)</code>  <code>staticmethod</code>","text":"<p>Compute the gradient of the tanh function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>gradient</code> <code>Tensor</code> <p>Gradient tensor.</p> Source code in <code>networks/Linear/feed_forward.py</code> <pre><code>@staticmethod\ndef get_gradient(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the gradient of the tanh function.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        gradient (Tensor): Gradient tensor.\n    \"\"\"\n    return  1 - torch.tanh(x)**2\n</code></pre>"}]}